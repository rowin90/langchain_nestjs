# langchain + ollama 本地模型对话

1. 启动 ollama，使用 http://localhost:11434
2. model可以使用的方法，invoke，stream等
